{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# feature selection based on the synthetic.py data_generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 5)\n",
      "118.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "synthetic.py:231: RuntimeWarning: covariance is not positive-semidefinite.\n",
      "  yK_ = np.random.multivariate_normal(ypheno[:, i], sigC * C1, size=1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from synthetic import generateData\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_train, y_train, Kva, Kve, beta = generateData(60, 80, 10, 2, 5, 0.5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "n_input = 80\n",
    "n_class = 5\n",
    "\n",
    "n_hidden_1 = 50\n",
    "n_hidden_2 = 5\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "training_epochs = 50000\n",
    "\n",
    "#lasso loss\n",
    "Alpha = 0.5\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_class])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## lasso weight matrix & bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "diag_random = np.random.normal(0, 0.1, n_input)\n",
    "Ma_diag = np.diag(diag_random)\n",
    "weight_lasso = tf.Variable(Ma_diag)\n",
    "weight_lasso = tf.cast(weight_lasso, tf.float32)\n",
    "\n",
    "bias_lasso = tf.Variable(tf.random_normal([n_input]))\n",
    "output_lasso = tf.add(tf.matmul(x, weight_lasso), bias_lasso)\n",
    "\n",
    "layer_0 = tf.nn.relu(output_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## add layer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def add_layer(inputs, a, b, act_Fun = None):\n",
    "    \n",
    "    weight = tf.cast(tf.Variable(tf.random_normal([a, b])), tf.float32)\n",
    "    bias = tf.Variable(tf.random_normal([b]))\n",
    "    yy = tf.add(tf.matmul(inputs, weight), bias)\n",
    "    \n",
    "    if act_Fun is None:\n",
    "        output = yy        \n",
    "    else:\n",
    "        output = act_Fun(yy)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def multiplayer_perceptron(first_in):\n",
    "    \n",
    "    layer_1 = add_layer(first_in, n_input, n_hidden_1, act_Fun = tf.nn.relu)\n",
    "    \n",
    "    layer_2 = add_layer(layer_1, n_hidden_1, n_hidden_2, act_Fun = tf.nn.relu)\n",
    "\n",
    "    out_layer = add_layer(layer_2, n_hidden_2, n_class, act_Fun = tf.nn.softmax) \n",
    "    #act_Fun not softmax if use tf.nn.softmax_cross_entropy_with_logits\n",
    "    \n",
    "    return out_layer\n",
    "\n",
    "pred = multiplayer_perceptron(layer_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#cal the sum of lasso M\n",
    "sess_1 = tf.Session()\n",
    "init_op=tf.global_variables_initializer()\n",
    "sess_1.run(init_op)\n",
    "weight_lasso = sess_1.run(weight_lasso)\n",
    "weight_lasso_sum = sum(map(sum,weight_lasso))\n",
    "\n",
    "loss = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred),1) + Alpha * weight_lasso_sum) \n",
    "optimize = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "train_loss = np.zeros(training_epochs)\n",
    "train_acc = np.zeros(training_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1  loss: 98589.773437500  TRAIN_ACCURACY: 0.233\n",
      "********************************************\n",
      "step: 2  loss: 74597.031250000  TRAIN_ACCURACY: 0.100\n",
      "********************************************\n",
      "step: 3  loss: 69838.078125000  TRAIN_ACCURACY: 0.083\n",
      "********************************************\n",
      "step: 4  loss: 47161.648437500  TRAIN_ACCURACY: 0.083\n",
      "********************************************\n",
      "step: 5  loss: 27032.318359375  TRAIN_ACCURACY: 0.100\n",
      "********************************************\n",
      "step: 6  loss: 24250.460937500  TRAIN_ACCURACY: 0.117\n",
      "********************************************\n",
      "step: 7  loss: 20969.583984375  TRAIN_ACCURACY: 0.100\n",
      "********************************************\n",
      "step: 8  loss: 15299.310546875  TRAIN_ACCURACY: 0.067\n",
      "********************************************\n",
      "step: 9  loss: 11850.787109375  TRAIN_ACCURACY: 0.050\n",
      "********************************************\n",
      "step: 10  loss: 11071.806640625  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 11  loss: 10777.895507812  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 12  loss: 10674.447265625  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 13  loss: 10640.269531250  TRAIN_ACCURACY: 0.033\n",
      "********************************************\n",
      "step: 14  loss: 10619.130859375  TRAIN_ACCURACY: 0.033\n",
      "********************************************\n",
      "step: 15  loss: 10593.587890625  TRAIN_ACCURACY: 0.033\n",
      "********************************************\n",
      "step: 16  loss: 10568.438476562  TRAIN_ACCURACY: 0.033\n",
      "********************************************\n",
      "step: 17  loss: 10544.302734375  TRAIN_ACCURACY: 0.033\n",
      "********************************************\n",
      "step: 18  loss: 10521.122070312  TRAIN_ACCURACY: 0.033\n",
      "********************************************\n",
      "step: 19  loss: 10498.893554688  TRAIN_ACCURACY: 0.033\n",
      "********************************************\n",
      "step: 20  loss: 10477.708007812  TRAIN_ACCURACY: 0.033\n",
      "********************************************\n",
      "step: 21  loss: 10457.795898438  TRAIN_ACCURACY: 0.033\n",
      "********************************************\n",
      "step: 22  loss: 10439.581054688  TRAIN_ACCURACY: 0.033\n",
      "********************************************\n",
      "step: 23  loss: 10423.676757812  TRAIN_ACCURACY: 0.033\n",
      "********************************************\n",
      "step: 24  loss: 10410.739257812  TRAIN_ACCURACY: 0.033\n",
      "********************************************\n",
      "step: 25  loss: 10401.135742188  TRAIN_ACCURACY: 0.033\n",
      "********************************************\n",
      "step: 26  loss: 10394.665039062  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 27  loss: 10390.613281250  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 28  loss: 10388.110351562  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 29  loss: 10385.753906250  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 30  loss: 10381.591796875  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 31  loss: 10377.406250000  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 32  loss: 10373.206054688  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 33  loss: 10368.989257812  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 34  loss: 10364.760742188  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 35  loss: 10360.519531250  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 36  loss: 10356.266601562  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 37  loss: 10352.005859375  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 38  loss: 10347.737304688  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 39  loss: 10343.459960938  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 40  loss: 10339.180664062  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 41  loss: 10334.892578125  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 42  loss: 10330.602539062  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 43  loss: 10326.307617188  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 44  loss: 10322.011718750  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 45  loss: 10317.711914062  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 46  loss: 10313.410156250  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 47  loss: 10309.106445312  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 48  loss: 10304.803710938  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 49  loss: 10300.500976562  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 50  loss: 10296.196289062  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 51  loss: 10291.891601562  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 52  loss: 10287.589843750  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 53  loss: 10283.287109375  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 54  loss: 10278.987304688  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 55  loss: 10274.689453125  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 56  loss: 10270.391601562  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 57  loss: 10266.095703125  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 58  loss: 10261.801757812  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 59  loss: 10257.510742188  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 60  loss: 10253.222656250  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 61  loss: 10248.936523438  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 62  loss: 10244.653320312  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 63  loss: 10240.371093750  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 64  loss: 10236.093750000  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 65  loss: 10231.819335938  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 66  loss: 10227.547851562  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 67  loss: 10223.279296875  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 68  loss: 10219.014648438  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 69  loss: 10214.752929688  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 70  loss: 10210.497070312  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 71  loss: 10206.241210938  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 72  loss: 10201.991210938  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 73  loss: 10197.744140625  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 74  loss: 10193.500976562  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 75  loss: 10189.261718750  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 76  loss: 10185.026367188  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 77  loss: 10180.796875000  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 78  loss: 10176.568359375  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 79  loss: 10172.343750000  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 80  loss: 10168.125000000  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 81  loss: 10163.910156250  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 82  loss: 10159.700195312  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 83  loss: 10155.494140625  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 84  loss: 10151.291015625  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 85  loss: 10147.093750000  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 86  loss: 10142.900390625  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 87  loss: 10138.709960938  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 88  loss: 10134.525390625  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 89  loss: 10130.345703125  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 90  loss: 10126.169921875  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 91  loss: 10121.998046875  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 92  loss: 10117.831054688  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 93  loss: 10113.667968750  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 94  loss: 10109.508789062  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 95  loss: 10105.356445312  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 96  loss: 10101.208007812  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 97  loss: 10097.062500000  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 98  loss: 10092.922851562  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 99  loss: 10088.788085938  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 100  loss: 10084.657226562  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 101  loss: 10080.533203125  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 102  loss: 10076.412109375  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 103  loss: 10072.295898438  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 104  loss: 10068.183593750  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 105  loss: 10064.077148438  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 106  loss: 10059.974609375  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 107  loss: 10055.877929688  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 108  loss: 10051.786132812  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 109  loss: 10047.700195312  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 110  loss: 10043.616210938  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 111  loss: 10039.540039062  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 112  loss: 10035.466796875  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 113  loss: 10031.397460938  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 114  loss: 10027.336914062  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 115  loss: 10023.277343750  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 116  loss: 10019.224609375  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 117  loss: 10015.176757812  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 118  loss: 10011.133789062  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 119  loss: 10007.094726562  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 120  loss: 10003.060546875  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 121  loss: 9999.033203125  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 122  loss: 9995.010742188  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 123  loss: 9990.991210938  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 124  loss: 9986.979492188  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 125  loss: 9982.969726562  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 126  loss: 9978.967773438  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 127  loss: 9974.968750000  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 128  loss: 9970.977539062  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 129  loss: 9966.988281250  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 130  loss: 9963.005859375  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 131  loss: 9959.028320312  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 132  loss: 9955.055664062  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 133  loss: 9951.089843750  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 134  loss: 9947.125000000  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 135  loss: 9943.167968750  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 136  loss: 9939.216796875  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 137  loss: 9935.267578125  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 138  loss: 9931.328125000  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 139  loss: 9927.390625000  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 140  loss: 9923.458007812  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 141  loss: 9919.531250000  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 142  loss: 9915.611328125  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 143  loss: 9911.693359375  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 144  loss: 9907.784179688  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 145  loss: 9903.877929688  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 146  loss: 9899.977539062  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 147  loss: 9896.082031250  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 148  loss: 9892.192382812  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 149  loss: 9888.307617188  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 150  loss: 9884.428710938  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 151  loss: 9880.553710938  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 152  loss: 9876.685546875  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 153  loss: 9872.822265625  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 154  loss: 9868.963867188  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 155  loss: 9865.109375000  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 156  loss: 9861.261718750  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 157  loss: 9857.419921875  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 158  loss: 9853.581054688  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 159  loss: 9849.750000000  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 160  loss: 9845.922851562  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 161  loss: 9842.102539062  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 162  loss: 9838.285156250  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 163  loss: 9834.474609375  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 164  loss: 9830.668945312  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 165  loss: 9826.869140625  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 166  loss: 9823.073242188  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 167  loss: 9819.284179688  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 168  loss: 9815.499023438  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 169  loss: 9811.720703125  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 170  loss: 9807.948242188  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 171  loss: 9804.177734375  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 172  loss: 9800.416992188  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 173  loss: 9796.657226562  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 174  loss: 9792.905273438  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 175  loss: 9789.158203125  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 176  loss: 9785.416992188  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 177  loss: 9781.680664062  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 178  loss: 9777.950195312  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 179  loss: 9774.224609375  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 180  loss: 9770.503906250  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 181  loss: 9766.790039062  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 182  loss: 9763.079101562  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 183  loss: 9759.376953125  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 184  loss: 9755.676757812  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 185  loss: 9751.985351562  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 186  loss: 9748.296875000  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 187  loss: 9744.614257812  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 188  loss: 9740.937500000  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 189  loss: 9737.264648438  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 190  loss: 9733.599609375  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 191  loss: 9729.939453125  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 192  loss: 9726.283203125  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 193  loss: 9722.633789062  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 194  loss: 9718.989257812  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 195  loss: 9715.348632812  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 196  loss: 9711.715820312  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 197  loss: 9708.087890625  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 198  loss: 9704.464843750  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 199  loss: 9700.846679688  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 200  loss: 9697.234375000  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 201  loss: 9693.626953125  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 202  loss: 9690.027343750  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 203  loss: 9686.431640625  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 204  loss: 9682.841796875  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 205  loss: 9679.255859375  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 206  loss: 9675.676757812  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 207  loss: 9672.103515625  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 208  loss: 9668.534179688  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 209  loss: 9664.970703125  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 210  loss: 9661.413085938  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 211  loss: 9657.860351562  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 212  loss: 9654.313476562  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 213  loss: 9650.772460938  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 214  loss: 9647.237304688  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 215  loss: 9643.706054688  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 216  loss: 9640.181640625  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 217  loss: 9636.660156250  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 218  loss: 9633.146484375  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 219  loss: 9629.637695312  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 220  loss: 9626.135742188  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 221  loss: 9622.637695312  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 222  loss: 9619.144531250  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 223  loss: 9615.658203125  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 224  loss: 9612.175781250  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 225  loss: 9608.700195312  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 226  loss: 9605.230468750  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 227  loss: 9601.764648438  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 228  loss: 9598.305664062  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 229  loss: 9594.849609375  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 230  loss: 9591.402343750  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 231  loss: 9587.958007812  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 232  loss: 9584.520507812  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 233  loss: 9581.088867188  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 234  loss: 9577.660156250  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 235  loss: 9574.239257812  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 236  loss: 9570.823242188  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 237  loss: 9567.413085938  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 238  loss: 9564.006835938  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 239  loss: 9560.608398438  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 240  loss: 9557.214843750  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 241  loss: 9553.825195312  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 242  loss: 9550.442382812  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 243  loss: 9547.063476562  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 244  loss: 9543.691406250  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 245  loss: 9540.325195312  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 246  loss: 9536.962890625  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 247  loss: 9533.606445312  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 248  loss: 9530.255859375  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 249  loss: 9526.912109375  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 250  loss: 9523.572265625  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 251  loss: 9520.236328125  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 252  loss: 9516.909179688  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 253  loss: 9513.584960938  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 254  loss: 9510.267578125  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 255  loss: 9506.954101562  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 256  loss: 9503.647460938  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 257  loss: 9500.345703125  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 258  loss: 9497.049804688  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 259  loss: 9493.758789062  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 260  loss: 9490.472656250  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 261  loss: 9487.193359375  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 262  loss: 9483.918945312  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 263  loss: 9480.650390625  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 264  loss: 9477.385742188  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 265  loss: 9474.128906250  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 266  loss: 9470.875000000  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 267  loss: 9467.628906250  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 268  loss: 9464.386718750  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 269  loss: 9461.150390625  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 270  loss: 9457.919921875  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 271  loss: 9454.694335938  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 272  loss: 9451.473632812  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 273  loss: 9448.258789062  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 274  loss: 9445.049804688  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 275  loss: 9441.845703125  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 276  loss: 9438.647460938  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 277  loss: 9435.455078125  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 278  loss: 9432.266601562  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 279  loss: 9429.084960938  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 280  loss: 9425.908203125  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 281  loss: 9422.737304688  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 282  loss: 9419.571289062  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 283  loss: 9416.410156250  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 284  loss: 9413.255859375  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 285  loss: 9410.105468750  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 286  loss: 9406.961914062  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 287  loss: 9403.823242188  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 288  loss: 9400.689453125  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 289  loss: 9397.560546875  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 290  loss: 9394.438476562  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 291  loss: 9391.322265625  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 292  loss: 9388.208007812  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 293  loss: 9385.102539062  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 294  loss: 9382.000000000  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 295  loss: 9378.904296875  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 296  loss: 9375.814453125  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 297  loss: 9372.731445312  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 298  loss: 9369.649414062  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 299  loss: 9366.576171875  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 300  loss: 9363.505859375  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 301  loss: 9360.443359375  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 302  loss: 9357.384765625  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 303  loss: 9354.332031250  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 304  loss: 9351.285156250  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 305  loss: 9348.243164062  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 306  loss: 9345.207031250  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 307  loss: 9342.174804688  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 308  loss: 9339.149414062  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 309  loss: 9336.127929688  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 310  loss: 9333.112304688  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 311  loss: 9330.104492188  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 312  loss: 9327.097656250  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 313  loss: 9324.097656250  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 314  loss: 9321.104492188  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 315  loss: 9318.116210938  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 316  loss: 9315.133789062  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 317  loss: 9312.155273438  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 318  loss: 9309.182617188  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 319  loss: 9306.215820312  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 320  loss: 9303.251953125  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 321  loss: 9300.295898438  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 322  loss: 9297.344726562  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 323  loss: 9294.400390625  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 324  loss: 9291.458007812  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 325  loss: 9288.522460938  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 326  loss: 9285.591796875  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 327  loss: 9282.667968750  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 328  loss: 9279.748046875  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 329  loss: 9276.833984375  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 330  loss: 9273.923828125  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 331  loss: 9271.020507812  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 332  loss: 9268.123046875  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 333  loss: 9265.229492188  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 334  loss: 9262.341796875  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 335  loss: 9259.458007812  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 336  loss: 9256.581054688  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 337  loss: 9253.708007812  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 338  loss: 9250.841796875  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 339  loss: 9247.981445312  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 340  loss: 9245.124023438  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 341  loss: 9242.272460938  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 342  loss: 9239.427734375  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 343  loss: 9236.584960938  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 344  loss: 9233.750976562  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 345  loss: 9230.920898438  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 346  loss: 9228.095703125  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 347  loss: 9225.277343750  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 348  loss: 9222.461914062  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 349  loss: 9219.653320312  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 350  loss: 9216.848632812  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 351  loss: 9214.049804688  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 352  loss: 9211.255859375  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 353  loss: 9208.466796875  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 354  loss: 9205.684570312  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 355  loss: 9202.907226562  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 356  loss: 9200.133789062  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 357  loss: 9197.366210938  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 358  loss: 9194.604492188  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 359  loss: 9191.846679688  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 360  loss: 9189.094726562  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 361  loss: 9186.347656250  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 362  loss: 9183.605468750  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 363  loss: 9180.869140625  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 364  loss: 9178.137695312  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 365  loss: 9175.412109375  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 366  loss: 9172.690429688  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 367  loss: 9169.974609375  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 368  loss: 9167.264648438  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 369  loss: 9164.558593750  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 370  loss: 9161.858398438  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 371  loss: 9159.161132812  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 372  loss: 9156.471679688  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 373  loss: 9153.787109375  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 374  loss: 9151.106445312  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 375  loss: 9148.432617188  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 376  loss: 9145.762695312  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 377  loss: 9143.096679688  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 378  loss: 9140.439453125  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 379  loss: 9137.783203125  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 380  loss: 9135.133789062  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 381  loss: 9132.490234375  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 382  loss: 9129.852539062  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 383  loss: 9127.216796875  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 384  loss: 9124.588867188  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 385  loss: 9121.963867188  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 386  loss: 9119.345703125  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 387  loss: 9116.731445312  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 388  loss: 9114.123046875  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 389  loss: 9111.518554688  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 390  loss: 9108.919921875  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 391  loss: 9106.327148438  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 392  loss: 9103.738281250  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 393  loss: 9101.155273438  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 394  loss: 9098.575195312  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 395  loss: 9096.002929688  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 396  loss: 9093.435546875  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 397  loss: 9090.870117188  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 398  loss: 9088.311523438  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 399  loss: 9085.758789062  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 400  loss: 9083.209960938  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 401  loss: 9080.666992188  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 402  loss: 9078.128906250  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 403  loss: 9075.593750000  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 404  loss: 9073.066406250  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 405  loss: 9070.543945312  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 406  loss: 9068.025390625  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 407  loss: 9065.510742188  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 408  loss: 9063.001953125  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 409  loss: 9060.500000000  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 410  loss: 9058.000976562  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 411  loss: 9055.506835938  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 412  loss: 9053.018554688  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 413  loss: 9050.535156250  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 414  loss: 9048.056640625  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 415  loss: 9045.583007812  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 416  loss: 9043.112304688  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 417  loss: 9040.647460938  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 418  loss: 9038.189453125  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 419  loss: 9035.737304688  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 420  loss: 9033.287109375  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 421  loss: 9030.841796875  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 422  loss: 9028.402343750  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 423  loss: 9025.968750000  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 424  loss: 9023.537109375  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 425  loss: 9021.112304688  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 426  loss: 9018.693359375  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 427  loss: 9016.279296875  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 428  loss: 9013.866210938  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 429  loss: 9011.462890625  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 430  loss: 9009.062500000  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 431  loss: 9006.666992188  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 432  loss: 9004.276367188  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 433  loss: 9001.889648438  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 434  loss: 8999.509765625  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 435  loss: 8997.130859375  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 436  loss: 8994.760742188  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 437  loss: 8992.393554688  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 438  loss: 8990.031250000  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 439  loss: 8987.674804688  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 440  loss: 8985.323242188  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 441  loss: 8982.977539062  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 442  loss: 8980.633789062  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 443  loss: 8978.295898438  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 444  loss: 8975.962890625  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 445  loss: 8973.633789062  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 446  loss: 8971.310546875  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 447  loss: 8968.991210938  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 448  loss: 8966.677734375  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 449  loss: 8964.369140625  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 450  loss: 8962.064453125  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 451  loss: 8959.764648438  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 452  loss: 8957.468750000  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 453  loss: 8955.180664062  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 454  loss: 8952.892578125  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 455  loss: 8950.612304688  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 456  loss: 8948.334960938  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 457  loss: 8946.065429688  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 458  loss: 8943.797851562  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 459  loss: 8941.535156250  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 460  loss: 8939.279296875  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 461  loss: 8937.027343750  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 462  loss: 8934.779296875  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 463  loss: 8932.535156250  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 464  loss: 8930.297851562  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 465  loss: 8928.062500000  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 466  loss: 8925.833984375  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 467  loss: 8923.609375000  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 468  loss: 8921.389648438  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 469  loss: 8919.172851562  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 470  loss: 8916.963867188  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 471  loss: 8914.758789062  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 472  loss: 8912.556640625  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 473  loss: 8910.360351562  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 474  loss: 8908.166992188  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 475  loss: 8905.979492188  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 476  loss: 8903.796875000  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 477  loss: 8901.618164062  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 478  loss: 8899.443359375  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 479  loss: 8897.275390625  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 480  loss: 8895.110351562  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 481  loss: 8892.950195312  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 482  loss: 8890.795898438  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 483  loss: 8888.643554688  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 484  loss: 8886.498046875  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 485  loss: 8884.356445312  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 486  loss: 8882.218750000  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 487  loss: 8880.087890625  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 488  loss: 8877.958007812  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 489  loss: 8875.833984375  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 490  loss: 8873.716796875  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 491  loss: 8871.600585938  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 492  loss: 8869.491210938  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 493  loss: 8867.385742188  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 494  loss: 8865.285156250  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 495  loss: 8863.187500000  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 496  loss: 8861.095703125  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 497  loss: 8859.008789062  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 498  loss: 8856.925781250  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 499  loss: 8854.847656250  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "完成！！！！！\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "     \n",
    "    step = 1\n",
    "    while step * batch_size < training_epochs:\n",
    "        \n",
    "        sess.run(optimize, feed_dict = {x: x_train, y: y_train})\n",
    "        \n",
    "        if step % display_step == 0:\n",
    "            \n",
    "            los, acc = sess.run([loss, accuracy], feed_dict={x: x_train, y: y_train})\n",
    "            \n",
    "            train_loss[step * batch_size] = los\n",
    "            train_acc[step * batch_size] = acc\n",
    "            \n",
    "            print(\"step: %d  loss: %.9f  TRAIN_ACCURACY: %.3f\"  % (step, los, acc))\n",
    "            \n",
    "            print \"********************************************\"\n",
    "            \n",
    "        step = step + 1\n",
    "    \n",
    "    \n",
    "    print \"完成！！！！！\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAEKCAYAAADEovgeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVjgnGzMzK9z/B9qpA1WsjZGLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1220385d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(211)\n",
    "plt.plot(train_loss, 'r')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Training loss\")\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel('Training Accuracy')\n",
    "plt.ylim(0.0, 1)\n",
    "plt.plot(train_acc, 'r')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
