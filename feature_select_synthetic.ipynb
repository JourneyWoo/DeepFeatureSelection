{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# feature selection based on the synthetic.py data_generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 5)\n",
      "118.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "synthetic.py:231: RuntimeWarning: covariance is not positive-semidefinite.\n",
      "  yK_ = np.random.multivariate_normal(ypheno[:, i], sigC * C1, size=1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from synthetic import generateData\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_train, y_train, Kva, Kve, beta = generateData(60, 80, 10, 2, 5, 0.5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "n_input = 80\n",
    "n_class = 5\n",
    "\n",
    "n_hidden_1 = 50\n",
    "n_hidden_2 = 5\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "training_epochs = 50000\n",
    "\n",
    "#lasso loss\n",
    "Alpha = 0.5\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_class])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## lasso weight matrix & bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "diag_random = np.random.normal(0, 0.1, n_input)\n",
    "Ma_diag = np.diag(diag_random)\n",
    "weight_lasso = tf.Variable(Ma_diag)\n",
    "weight_lasso = tf.cast(weight_lasso, tf.float32)\n",
    "\n",
    "bias_lasso = tf.Variable(tf.random_normal([n_input]))\n",
    "output_lasso = tf.add(tf.matmul(x, weight_lasso), bias_lasso)\n",
    "\n",
    "layer_0 = tf.nn.relu(output_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## add layer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def add_layer(inputs, a, b, act_Fun = None):\n",
    "    \n",
    "    weight = tf.cast(tf.Variable(tf.random_normal([a, b])), tf.float32)\n",
    "    bias = tf.Variable(tf.random_normal([b]))\n",
    "    yy = tf.add(tf.matmul(inputs, weight), bias)\n",
    "    \n",
    "    if act_Fun is None:\n",
    "        output = yy        \n",
    "    else:\n",
    "        output = act_Fun(yy)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def multiplayer_perceptron(first_in):\n",
    "    \n",
    "    layer_1 = add_layer(first_in, n_input, n_hidden_1, act_Fun = tf.nn.relu)\n",
    "    \n",
    "    layer_2 = add_layer(layer_1, n_hidden_1, n_hidden_2, act_Fun = tf.nn.relu)\n",
    "\n",
    "    out_layer = add_layer(layer_2, n_hidden_2, n_class, act_Fun = tf.nn.softmax) \n",
    "    #act_Fun not softmax if use tf.nn.softmax_cross_entropy_with_logits\n",
    "    \n",
    "    return out_layer\n",
    "\n",
    "pred = multiplayer_perceptron(layer_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#cal the sum of lasso M\n",
    "sess_1 = tf.Session()\n",
    "init_op=tf.global_variables_initializer()\n",
    "sess_1.run(init_op)\n",
    "weight_lasso = sess_1.run(weight_lasso)\n",
    "weight_lasso_sum = sum(map(sum,weight_lasso))\n",
    "\n",
    "loss = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred),1) + Alpha * weight_lasso_sum) \n",
    "optimize = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "train_loss = np.zeros(training_epochs)\n",
    "train_acc = np.zeros(training_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1  loss: 14144.766601562  TRAIN_ACCURACY: 0.033\n",
      "********************************************\n",
      "step: 2  loss: 12499.848632812  TRAIN_ACCURACY: 0.000\n",
      "********************************************\n",
      "step: 3  loss: 12495.110351562  TRAIN_ACCURACY: 0.000\n",
      "********************************************\n",
      "step: 4  loss: 12490.279296875  TRAIN_ACCURACY: 0.000\n",
      "********************************************\n",
      "step: 5  loss: 12485.385742188  TRAIN_ACCURACY: 0.000\n",
      "********************************************\n",
      "step: 6  loss: 12460.031250000  TRAIN_ACCURACY: 0.000\n",
      "********************************************\n",
      "step: 7  loss: 12466.358398438  TRAIN_ACCURACY: 0.000\n",
      "********************************************\n",
      "step: 8  loss: 12464.962890625  TRAIN_ACCURACY: 0.000\n",
      "********************************************\n",
      "step: 9  loss: 12489.956054688  TRAIN_ACCURACY: 0.000\n",
      "********************************************\n",
      "step: 10  loss: 12500.126953125  TRAIN_ACCURACY: 0.017\n",
      "********************************************\n",
      "step: 11  loss: 12481.495117188  TRAIN_ACCURACY: 0.033\n",
      "********************************************\n",
      "step: 12  loss: 12440.052734375  TRAIN_ACCURACY: 0.033\n",
      "********************************************\n",
      "step: 13  loss: 12380.740234375  TRAIN_ACCURACY: 0.050\n",
      "********************************************\n",
      "step: 14  loss: 12323.104492188  TRAIN_ACCURACY: 0.050\n",
      "********************************************\n",
      "step: 15  loss: 12277.251953125  TRAIN_ACCURACY: 0.050\n",
      "********************************************\n",
      "step: 16  loss: 12250.005859375  TRAIN_ACCURACY: 0.067\n",
      "********************************************\n",
      "step: 17  loss: 12236.481445312  TRAIN_ACCURACY: 0.067\n",
      "********************************************\n",
      "step: 18  loss: 12212.970703125  TRAIN_ACCURACY: 0.067\n",
      "********************************************\n",
      "step: 19  loss: 12190.385742188  TRAIN_ACCURACY: 0.083\n",
      "********************************************\n",
      "step: 20  loss: 12168.668945312  TRAIN_ACCURACY: 0.083\n",
      "********************************************\n",
      "step: 21  loss: 12187.024414062  TRAIN_ACCURACY: 0.100\n",
      "********************************************\n",
      "step: 22  loss: 12169.872070312  TRAIN_ACCURACY: 0.100\n",
      "********************************************\n",
      "step: 23  loss: 12130.639648438  TRAIN_ACCURACY: 0.100\n",
      "********************************************\n",
      "step: 24  loss: 12107.477539062  TRAIN_ACCURACY: 0.083\n",
      "********************************************\n",
      "step: 25  loss: 12097.983398438  TRAIN_ACCURACY: 0.083\n",
      "********************************************\n",
      "step: 26  loss: 12063.689453125  TRAIN_ACCURACY: 0.100\n",
      "********************************************\n",
      "step: 27  loss: 12037.277343750  TRAIN_ACCURACY: 0.100\n",
      "********************************************\n",
      "step: 28  loss: 12026.181640625  TRAIN_ACCURACY: 0.100\n",
      "********************************************\n",
      "step: 29  loss: 12008.422851562  TRAIN_ACCURACY: 0.100\n",
      "********************************************\n",
      "step: 30  loss: 11988.962890625  TRAIN_ACCURACY: 0.100\n",
      "********************************************\n",
      "step: 31  loss: 11982.997070312  TRAIN_ACCURACY: 0.100\n",
      "********************************************\n",
      "step: 32  loss: 11974.009765625  TRAIN_ACCURACY: 0.100\n",
      "********************************************\n",
      "step: 33  loss: 11960.599609375  TRAIN_ACCURACY: 0.100\n",
      "********************************************\n",
      "step: 34  loss: 11945.045898438  TRAIN_ACCURACY: 0.100\n",
      "********************************************\n",
      "step: 35  loss: 11931.962890625  TRAIN_ACCURACY: 0.133\n",
      "********************************************\n",
      "step: 36  loss: 11911.866210938  TRAIN_ACCURACY: 0.133\n",
      "********************************************\n",
      "step: 37  loss: 11883.714843750  TRAIN_ACCURACY: 0.133\n",
      "********************************************\n",
      "step: 38  loss: 11862.547851562  TRAIN_ACCURACY: 0.133\n",
      "********************************************\n",
      "step: 39  loss: 11847.090820312  TRAIN_ACCURACY: 0.133\n",
      "********************************************\n",
      "step: 40  loss: 11828.637695312  TRAIN_ACCURACY: 0.133\n",
      "********************************************\n",
      "step: 41  loss: 11817.477539062  TRAIN_ACCURACY: 0.133\n",
      "********************************************\n",
      "step: 42  loss: 11805.530273438  TRAIN_ACCURACY: 0.133\n",
      "********************************************\n",
      "step: 43  loss: 11791.982421875  TRAIN_ACCURACY: 0.133\n",
      "********************************************\n",
      "step: 44  loss: 11782.983398438  TRAIN_ACCURACY: 0.133\n",
      "********************************************\n",
      "step: 45  loss: 11771.189453125  TRAIN_ACCURACY: 0.133\n",
      "********************************************\n",
      "step: 46  loss: 11760.863281250  TRAIN_ACCURACY: 0.133\n",
      "********************************************\n",
      "step: 47  loss: 11752.327148438  TRAIN_ACCURACY: 0.133\n",
      "********************************************\n",
      "step: 48  loss: 11741.485351562  TRAIN_ACCURACY: 0.133\n",
      "********************************************\n",
      "step: 49  loss: 11733.149414062  TRAIN_ACCURACY: 0.133\n",
      "********************************************\n",
      "step: 50  loss: 11723.985351562  TRAIN_ACCURACY: 0.133\n",
      "********************************************\n",
      "step: 51  loss: 11714.031250000  TRAIN_ACCURACY: 0.133\n",
      "********************************************\n",
      "step: 52  loss: 11705.709960938  TRAIN_ACCURACY: 0.133\n",
      "********************************************\n",
      "step: 53  loss: 11695.556640625  TRAIN_ACCURACY: 0.133\n",
      "********************************************\n",
      "step: 54  loss: 11685.965820312  TRAIN_ACCURACY: 0.133\n",
      "********************************************\n",
      "step: 55  loss: 11676.411132812  TRAIN_ACCURACY: 0.133\n",
      "********************************************\n",
      "step: 56  loss: 11666.001953125  TRAIN_ACCURACY: 0.133\n",
      "********************************************\n",
      "step: 57  loss: 11656.903320312  TRAIN_ACCURACY: 0.133\n",
      "********************************************\n",
      "step: 58  loss: 11647.172851562  TRAIN_ACCURACY: 0.133\n",
      "********************************************\n",
      "step: 59  loss: 11637.866210938  TRAIN_ACCURACY: 0.133\n",
      "********************************************\n",
      "step: 60  loss: 11628.938476562  TRAIN_ACCURACY: 0.133\n",
      "********************************************\n",
      "step: 61  loss: 11619.389648438  TRAIN_ACCURACY: 0.133\n",
      "********************************************\n",
      "step: 62  loss: 11610.606445312  TRAIN_ACCURACY: 0.133\n",
      "********************************************\n",
      "step: 63  loss: 11601.468750000  TRAIN_ACCURACY: 0.133\n",
      "********************************************\n",
      "step: 64  loss: 11592.620117188  TRAIN_ACCURACY: 0.133\n",
      "********************************************\n",
      "step: 65  loss: 11584.120117188  TRAIN_ACCURACY: 0.133\n",
      "********************************************\n",
      "step: 66  loss: 11575.337890625  TRAIN_ACCURACY: 0.133\n",
      "********************************************\n",
      "step: 67  loss: 11567.049804688  TRAIN_ACCURACY: 0.133\n",
      "********************************************\n",
      "step: 68  loss: 11558.447265625  TRAIN_ACCURACY: 0.133\n",
      "********************************************\n",
      "step: 69  loss: 11549.935546875  TRAIN_ACCURACY: 0.133\n",
      "********************************************\n",
      "step: 70  loss: 11541.421875000  TRAIN_ACCURACY: 0.133\n",
      "********************************************\n",
      "step: 71  loss: 11532.652343750  TRAIN_ACCURACY: 0.133\n",
      "********************************************\n",
      "step: 72  loss: 11524.083007812  TRAIN_ACCURACY: 0.133\n",
      "********************************************\n",
      "step: 73  loss: 11515.261718750  TRAIN_ACCURACY: 0.133\n",
      "********************************************\n",
      "step: 74  loss: 11506.583007812  TRAIN_ACCURACY: 0.133\n",
      "********************************************\n",
      "step: 75  loss: 11497.905273438  TRAIN_ACCURACY: 0.133\n",
      "********************************************\n",
      "step: 76  loss: 11489.244140625  TRAIN_ACCURACY: 0.133\n",
      "********************************************\n",
      "step: 77  loss: 11480.796875000  TRAIN_ACCURACY: 0.133\n",
      "********************************************\n",
      "step: 78  loss: 11472.299804688  TRAIN_ACCURACY: 0.133\n",
      "********************************************\n",
      "step: 79  loss: 11463.986328125  TRAIN_ACCURACY: 0.133\n",
      "********************************************\n",
      "step: 80  loss: 11455.608398438  TRAIN_ACCURACY: 0.133\n",
      "********************************************\n",
      "step: 81  loss: 11447.243164062  TRAIN_ACCURACY: 0.133\n",
      "********************************************\n",
      "step: 82  loss: 11438.872070312  TRAIN_ACCURACY: 0.133\n",
      "********************************************\n",
      "step: 83  loss: 11430.428710938  TRAIN_ACCURACY: 0.133\n",
      "********************************************\n",
      "step: 84  loss: 11422.062500000  TRAIN_ACCURACY: 0.133\n",
      "********************************************\n",
      "step: 85  loss: 11413.650390625  TRAIN_ACCURACY: 0.133\n",
      "********************************************\n",
      "step: 86  loss: 11405.318359375  TRAIN_ACCURACY: 0.133\n",
      "********************************************\n",
      "step: 87  loss: 11396.980468750  TRAIN_ACCURACY: 0.133\n",
      "********************************************\n",
      "step: 88  loss: 11388.658203125  TRAIN_ACCURACY: 0.133\n",
      "********************************************\n",
      "step: 89  loss: 11380.354492188  TRAIN_ACCURACY: 0.133\n",
      "********************************************\n",
      "step: 90  loss: 11372.026367188  TRAIN_ACCURACY: 0.133\n",
      "********************************************\n",
      "step: 91  loss: 11363.110351562  TRAIN_ACCURACY: 0.133\n",
      "********************************************\n",
      "step: 92  loss: 11314.702148438  TRAIN_ACCURACY: 0.133\n",
      "********************************************\n",
      "step: 93  loss: 11258.576171875  TRAIN_ACCURACY: 0.150\n",
      "********************************************\n",
      "step: 94  loss: 11202.003906250  TRAIN_ACCURACY: 0.150\n",
      "********************************************\n",
      "step: 95  loss: 11109.397460938  TRAIN_ACCURACY: 0.150\n",
      "********************************************\n",
      "step: 96  loss: 11038.734375000  TRAIN_ACCURACY: 0.150\n",
      "********************************************\n",
      "step: 97  loss: 11014.494140625  TRAIN_ACCURACY: 0.150\n",
      "********************************************\n",
      "step: 98  loss: 11004.823242188  TRAIN_ACCURACY: 0.150\n",
      "********************************************\n",
      "step: 99  loss: 11001.737304688  TRAIN_ACCURACY: 0.150\n",
      "********************************************\n",
      "step: 100  loss: 10990.841796875  TRAIN_ACCURACY: 0.150\n",
      "********************************************\n",
      "step: 101  loss: 10971.542968750  TRAIN_ACCURACY: 0.150\n",
      "********************************************\n",
      "step: 102  loss: 10944.499023438  TRAIN_ACCURACY: 0.150\n",
      "********************************************\n",
      "step: 103  loss: 10911.147460938  TRAIN_ACCURACY: 0.150\n",
      "********************************************\n",
      "step: 104  loss: 10875.876953125  TRAIN_ACCURACY: 0.150\n",
      "********************************************\n",
      "step: 105  loss: 10850.068359375  TRAIN_ACCURACY: 0.150\n",
      "********************************************\n",
      "step: 106  loss: 10837.833007812  TRAIN_ACCURACY: 0.150\n",
      "********************************************\n",
      "step: 107  loss: 10830.613281250  TRAIN_ACCURACY: 0.150\n",
      "********************************************\n",
      "step: 108  loss: 10811.267578125  TRAIN_ACCURACY: 0.150\n",
      "********************************************\n",
      "step: 109  loss: 10772.030273438  TRAIN_ACCURACY: 0.150\n",
      "********************************************\n",
      "step: 110  loss: 10731.760742188  TRAIN_ACCURACY: 0.150\n",
      "********************************************\n",
      "step: 111  loss: 10696.431640625  TRAIN_ACCURACY: 0.150\n",
      "********************************************\n",
      "step: 112  loss: 10673.098632812  TRAIN_ACCURACY: 0.150\n",
      "********************************************\n",
      "step: 113  loss: 10649.022460938  TRAIN_ACCURACY: 0.150\n",
      "********************************************\n",
      "step: 114  loss: 10627.676757812  TRAIN_ACCURACY: 0.150\n",
      "********************************************\n",
      "step: 115  loss: 10605.718750000  TRAIN_ACCURACY: 0.150\n",
      "********************************************\n",
      "step: 116  loss: 10588.373046875  TRAIN_ACCURACY: 0.167\n",
      "********************************************\n",
      "step: 117  loss: 10572.961914062  TRAIN_ACCURACY: 0.167\n",
      "********************************************\n",
      "step: 118  loss: 10551.916992188  TRAIN_ACCURACY: 0.167\n",
      "********************************************\n",
      "step: 119  loss: 10532.605468750  TRAIN_ACCURACY: 0.167\n",
      "********************************************\n",
      "step: 120  loss: 10508.168945312  TRAIN_ACCURACY: 0.167\n",
      "********************************************\n",
      "step: 121  loss: 10488.814453125  TRAIN_ACCURACY: 0.167\n",
      "********************************************\n",
      "step: 122  loss: 10472.755859375  TRAIN_ACCURACY: 0.167\n",
      "********************************************\n",
      "step: 123  loss: 10458.596679688  TRAIN_ACCURACY: 0.167\n",
      "********************************************\n",
      "step: 124  loss: 10447.287109375  TRAIN_ACCURACY: 0.167\n",
      "********************************************\n",
      "step: 125  loss: 10432.428710938  TRAIN_ACCURACY: 0.167\n",
      "********************************************\n",
      "step: 126  loss: 10417.863281250  TRAIN_ACCURACY: 0.167\n",
      "********************************************\n",
      "step: 127  loss: 10399.145507812  TRAIN_ACCURACY: 0.167\n",
      "********************************************\n",
      "step: 128  loss: 10379.110351562  TRAIN_ACCURACY: 0.167\n",
      "********************************************\n",
      "step: 129  loss: 10352.625976562  TRAIN_ACCURACY: 0.167\n",
      "********************************************\n",
      "step: 130  loss: 10333.604492188  TRAIN_ACCURACY: 0.167\n",
      "********************************************\n",
      "step: 131  loss: 10319.255859375  TRAIN_ACCURACY: 0.167\n",
      "********************************************\n",
      "step: 132  loss: 10305.189453125  TRAIN_ACCURACY: 0.167\n",
      "********************************************\n",
      "step: 133  loss: 10295.009765625  TRAIN_ACCURACY: 0.167\n",
      "********************************************\n",
      "step: 134  loss: 10279.768554688  TRAIN_ACCURACY: 0.167\n",
      "********************************************\n",
      "step: 135  loss: 10265.616210938  TRAIN_ACCURACY: 0.167\n",
      "********************************************\n",
      "step: 136  loss: 10252.795898438  TRAIN_ACCURACY: 0.167\n",
      "********************************************\n",
      "step: 137  loss: 10238.584960938  TRAIN_ACCURACY: 0.167\n",
      "********************************************\n",
      "step: 138  loss: 10227.837890625  TRAIN_ACCURACY: 0.167\n",
      "********************************************\n",
      "step: 139  loss: 10207.569335938  TRAIN_ACCURACY: 0.167\n",
      "********************************************\n",
      "step: 140  loss: 10185.102539062  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 141  loss: 10166.821289062  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 142  loss: 10147.747070312  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 143  loss: 10124.928710938  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 144  loss: 10100.878906250  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 145  loss: 10084.246093750  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 146  loss: 10073.884765625  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 147  loss: 10060.543945312  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 148  loss: 10046.084960938  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 149  loss: 10031.053710938  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 150  loss: 10016.610351562  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 151  loss: 10006.979492188  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 152  loss: 9998.473632812  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 153  loss: 9987.413085938  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 154  loss: 9975.360351562  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 155  loss: 9961.885742188  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 156  loss: 9948.371093750  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 157  loss: 9937.591796875  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 158  loss: 9927.717773438  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 159  loss: 9917.041992188  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 160  loss: 9906.375000000  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 161  loss: 9895.333007812  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 162  loss: 9884.093750000  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 163  loss: 9873.946289062  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 164  loss: 9864.272460938  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 165  loss: 9854.139648438  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 166  loss: 9844.263671875  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 167  loss: 9834.886718750  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 168  loss: 9825.415039062  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 169  loss: 9815.900390625  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 170  loss: 9806.583007812  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 171  loss: 9797.237304688  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 172  loss: 9787.908203125  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 173  loss: 9778.918945312  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 174  loss: 9770.102539062  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 175  loss: 9761.131835938  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 176  loss: 9752.178710938  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 177  loss: 9743.575195312  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 178  loss: 9735.235351562  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 179  loss: 9726.892578125  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 180  loss: 9718.497070312  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 181  loss: 9710.141601562  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 182  loss: 9701.897460938  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 183  loss: 9693.792968750  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 184  loss: 9685.847656250  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 185  loss: 9678.000000000  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 186  loss: 9670.161132812  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 187  loss: 9662.333007812  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 188  loss: 9654.614257812  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 189  loss: 9647.047851562  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 190  loss: 9639.569335938  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 191  loss: 9632.098632812  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 192  loss: 9624.643554688  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 193  loss: 9617.280273438  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 194  loss: 9610.053710938  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 195  loss: 9602.931640625  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 196  loss: 9595.856445312  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 197  loss: 9588.803710938  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 198  loss: 9581.807617188  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 199  loss: 9574.893554688  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 200  loss: 9568.068359375  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 201  loss: 9561.310546875  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 202  loss: 9554.600585938  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 203  loss: 9547.940429688  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 204  loss: 9541.337890625  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 205  loss: 9534.801757812  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 206  loss: 9528.337890625  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 207  loss: 9521.935546875  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 208  loss: 9515.589843750  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 209  loss: 9509.292968750  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 210  loss: 9503.047851562  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 211  loss: 9496.854492188  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 212  loss: 9490.718750000  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 213  loss: 9484.636718750  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 214  loss: 9478.604492188  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 215  loss: 9472.625000000  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 216  loss: 9466.697265625  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 217  loss: 9460.824218750  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 218  loss: 9455.001953125  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 219  loss: 9449.228515625  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 220  loss: 9443.499023438  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 221  loss: 9437.818359375  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 222  loss: 9432.185546875  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 223  loss: 9426.602539062  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 224  loss: 9421.065429688  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 225  loss: 9415.580078125  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 226  loss: 9410.144531250  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 227  loss: 9404.769531250  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 228  loss: 9399.470703125  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 229  loss: 9394.287109375  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 230  loss: 9389.300781250  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 231  loss: 9384.735351562  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 232  loss: 9380.966796875  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 233  loss: 9379.303710938  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 234  loss: 9381.106445312  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 235  loss: 9393.248046875  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 236  loss: 9410.295898438  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 237  loss: 9448.724609375  TRAIN_ACCURACY: 0.333\n",
      "********************************************\n",
      "step: 238  loss: 9424.768554688  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 239  loss: 9390.467773438  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 240  loss: 9341.577148438  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 241  loss: 9345.366210938  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 242  loss: 9377.980468750  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 243  loss: 9363.310546875  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 244  loss: 9330.795898438  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 245  loss: 9315.204101562  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 246  loss: 9330.097656250  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 247  loss: 9339.168945312  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 248  loss: 9311.680664062  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 249  loss: 9295.979492188  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 250  loss: 9304.461914062  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 251  loss: 9305.378906250  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 252  loss: 9290.959960938  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 253  loss: 9277.735351562  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 254  loss: 9280.809570312  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 255  loss: 9283.597656250  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 256  loss: 9270.571289062  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 257  loss: 9260.200195312  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 258  loss: 9260.848632812  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 259  loss: 9260.170898438  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 260  loss: 9251.940429688  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 261  loss: 9243.170898438  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 262  loss: 9241.708007812  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 263  loss: 9241.093750000  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 264  loss: 9233.998046875  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 265  loss: 9226.610351562  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 266  loss: 9223.874023438  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 267  loss: 9222.121093750  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 268  loss: 9217.143554688  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 269  loss: 9210.610351562  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 270  loss: 9206.915039062  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 271  loss: 9204.810546875  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 272  loss: 9200.593750000  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 273  loss: 9195.037109375  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 274  loss: 9190.781250000  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 275  loss: 9188.041992188  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 276  loss: 9184.672851562  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 277  loss: 9179.862304688  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 278  loss: 9175.400390625  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 279  loss: 9172.141601562  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 280  loss: 9168.979492188  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 281  loss: 9164.974609375  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 282  loss: 9160.625000000  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 283  loss: 9156.949218750  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 284  loss: 9153.779296875  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 285  loss: 9150.255859375  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 286  loss: 9146.281250000  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 287  loss: 9142.433593750  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 288  loss: 9139.056640625  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 289  loss: 9135.785156250  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 290  loss: 9132.200195312  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 291  loss: 9128.454101562  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 292  loss: 9124.912109375  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 293  loss: 9121.626953125  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 294  loss: 9118.323242188  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 295  loss: 9114.826171875  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 296  loss: 9111.295898438  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 297  loss: 9107.926757812  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 298  loss: 9104.694335938  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 299  loss: 9101.427734375  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 300  loss: 9098.060546875  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 301  loss: 9094.698242188  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 302  loss: 9091.444335938  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 303  loss: 9088.270507812  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 304  loss: 9085.079101562  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 305  loss: 9081.837890625  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 306  loss: 9078.608398438  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 307  loss: 9075.451171875  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 308  loss: 9072.347656250  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 309  loss: 9069.245117188  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 310  loss: 9066.116210938  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 311  loss: 9063.001953125  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 312  loss: 9059.934570312  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 313  loss: 9056.909179688  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 314  loss: 9053.892578125  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 315  loss: 9050.871093750  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 316  loss: 9047.860351562  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 317  loss: 9044.879882812  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 318  loss: 9041.933593750  TRAIN_ACCURACY: 0.200\n",
      "********************************************\n",
      "step: 319  loss: 9039.003906250  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 320  loss: 9036.079101562  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 321  loss: 9033.165039062  TRAIN_ACCURACY: 0.200\n",
      "********************************************\n",
      "step: 322  loss: 9030.271484375  TRAIN_ACCURACY: 0.200\n",
      "********************************************\n",
      "step: 323  loss: 9027.406250000  TRAIN_ACCURACY: 0.200\n",
      "********************************************\n",
      "step: 324  loss: 9024.560546875  TRAIN_ACCURACY: 0.217\n",
      "********************************************\n",
      "step: 325  loss: 9021.724609375  TRAIN_ACCURACY: 0.200\n",
      "********************************************\n",
      "step: 326  loss: 9018.900390625  TRAIN_ACCURACY: 0.200\n",
      "********************************************\n",
      "step: 327  loss: 9016.094726562  TRAIN_ACCURACY: 0.200\n",
      "********************************************\n",
      "step: 328  loss: 9013.310546875  TRAIN_ACCURACY: 0.200\n",
      "********************************************\n",
      "step: 329  loss: 9010.546875000  TRAIN_ACCURACY: 0.217\n",
      "********************************************\n",
      "step: 330  loss: 9007.795898438  TRAIN_ACCURACY: 0.200\n",
      "********************************************\n",
      "step: 331  loss: 9005.058593750  TRAIN_ACCURACY: 0.200\n",
      "********************************************\n",
      "step: 332  loss: 9002.334960938  TRAIN_ACCURACY: 0.200\n",
      "********************************************\n",
      "step: 333  loss: 8999.631835938  TRAIN_ACCURACY: 0.200\n",
      "********************************************\n",
      "step: 334  loss: 8996.948242188  TRAIN_ACCURACY: 0.200\n",
      "********************************************\n",
      "step: 335  loss: 8994.280273438  TRAIN_ACCURACY: 0.200\n",
      "********************************************\n",
      "step: 336  loss: 8991.625000000  TRAIN_ACCURACY: 0.200\n",
      "********************************************\n",
      "step: 337  loss: 8988.983398438  TRAIN_ACCURACY: 0.200\n",
      "********************************************\n",
      "step: 338  loss: 8986.360351562  TRAIN_ACCURACY: 0.200\n",
      "********************************************\n",
      "step: 339  loss: 8983.753906250  TRAIN_ACCURACY: 0.200\n",
      "********************************************\n",
      "step: 340  loss: 8981.165039062  TRAIN_ACCURACY: 0.200\n",
      "********************************************\n",
      "step: 341  loss: 8978.589843750  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 342  loss: 8976.027343750  TRAIN_ACCURACY: 0.200\n",
      "********************************************\n",
      "step: 343  loss: 8973.481445312  TRAIN_ACCURACY: 0.167\n",
      "********************************************\n",
      "step: 344  loss: 8970.953125000  TRAIN_ACCURACY: 0.167\n",
      "********************************************\n",
      "step: 345  loss: 8968.440429688  TRAIN_ACCURACY: 0.167\n",
      "********************************************\n",
      "step: 346  loss: 8965.941406250  TRAIN_ACCURACY: 0.200\n",
      "********************************************\n",
      "step: 347  loss: 8963.458007812  TRAIN_ACCURACY: 0.183\n",
      "********************************************\n",
      "step: 348  loss: 8960.988281250  TRAIN_ACCURACY: 0.200\n",
      "********************************************\n",
      "step: 349  loss: 8958.535156250  TRAIN_ACCURACY: 0.200\n",
      "********************************************\n",
      "step: 350  loss: 8956.096679688  TRAIN_ACCURACY: 0.200\n",
      "********************************************\n",
      "step: 351  loss: 8953.674804688  TRAIN_ACCURACY: 0.217\n",
      "********************************************\n",
      "step: 352  loss: 8951.264648438  TRAIN_ACCURACY: 0.217\n",
      "********************************************\n",
      "step: 353  loss: 8948.871093750  TRAIN_ACCURACY: 0.217\n",
      "********************************************\n",
      "step: 354  loss: 8946.491210938  TRAIN_ACCURACY: 0.233\n",
      "********************************************\n",
      "step: 355  loss: 8944.126953125  TRAIN_ACCURACY: 0.233\n",
      "********************************************\n",
      "step: 356  loss: 8941.777343750  TRAIN_ACCURACY: 0.267\n",
      "********************************************\n",
      "step: 357  loss: 8939.441406250  TRAIN_ACCURACY: 0.267\n",
      "********************************************\n",
      "step: 358  loss: 8937.120117188  TRAIN_ACCURACY: 0.267\n",
      "********************************************\n",
      "step: 359  loss: 8934.812500000  TRAIN_ACCURACY: 0.267\n",
      "********************************************\n",
      "step: 360  loss: 8932.520507812  TRAIN_ACCURACY: 0.267\n",
      "********************************************\n",
      "step: 361  loss: 8930.241210938  TRAIN_ACCURACY: 0.267\n",
      "********************************************\n",
      "step: 362  loss: 8927.978515625  TRAIN_ACCURACY: 0.283\n",
      "********************************************\n",
      "step: 363  loss: 8925.728515625  TRAIN_ACCURACY: 0.283\n",
      "********************************************\n",
      "step: 364  loss: 8923.491210938  TRAIN_ACCURACY: 0.300\n",
      "********************************************\n",
      "step: 365  loss: 8921.270507812  TRAIN_ACCURACY: 0.317\n",
      "********************************************\n",
      "step: 366  loss: 8919.062500000  TRAIN_ACCURACY: 0.333\n",
      "********************************************\n",
      "step: 367  loss: 8916.868164062  TRAIN_ACCURACY: 0.333\n",
      "********************************************\n",
      "step: 368  loss: 8914.687500000  TRAIN_ACCURACY: 0.350\n",
      "********************************************\n",
      "step: 369  loss: 8912.520507812  TRAIN_ACCURACY: 0.350\n",
      "********************************************\n",
      "step: 370  loss: 8910.369140625  TRAIN_ACCURACY: 0.350\n",
      "********************************************\n",
      "step: 371  loss: 8908.230468750  TRAIN_ACCURACY: 0.333\n",
      "********************************************\n",
      "step: 372  loss: 8906.104492188  TRAIN_ACCURACY: 0.350\n",
      "********************************************\n",
      "step: 373  loss: 8903.993164062  TRAIN_ACCURACY: 0.350\n",
      "********************************************\n",
      "step: 374  loss: 8901.893554688  TRAIN_ACCURACY: 0.350\n",
      "********************************************\n",
      "step: 375  loss: 8899.809570312  TRAIN_ACCURACY: 0.350\n",
      "********************************************\n",
      "step: 376  loss: 8897.737304688  TRAIN_ACCURACY: 0.367\n",
      "********************************************\n",
      "step: 377  loss: 8895.678710938  TRAIN_ACCURACY: 0.367\n",
      "********************************************\n",
      "step: 378  loss: 8893.635742188  TRAIN_ACCURACY: 0.367\n",
      "********************************************\n",
      "step: 379  loss: 8891.603515625  TRAIN_ACCURACY: 0.383\n",
      "********************************************\n",
      "step: 380  loss: 8889.584960938  TRAIN_ACCURACY: 0.383\n",
      "********************************************\n",
      "step: 381  loss: 8887.580078125  TRAIN_ACCURACY: 0.383\n",
      "********************************************\n",
      "step: 382  loss: 8885.587890625  TRAIN_ACCURACY: 0.383\n",
      "********************************************\n",
      "step: 383  loss: 8883.608398438  TRAIN_ACCURACY: 0.383\n",
      "********************************************\n",
      "step: 384  loss: 8881.642578125  TRAIN_ACCURACY: 0.367\n",
      "********************************************\n",
      "step: 385  loss: 8879.689453125  TRAIN_ACCURACY: 0.367\n",
      "********************************************\n",
      "step: 386  loss: 8877.749023438  TRAIN_ACCURACY: 0.367\n",
      "********************************************\n",
      "step: 387  loss: 8875.822265625  TRAIN_ACCURACY: 0.367\n",
      "********************************************\n",
      "step: 388  loss: 8873.907226562  TRAIN_ACCURACY: 0.367\n",
      "********************************************\n",
      "step: 389  loss: 8872.004882812  TRAIN_ACCURACY: 0.367\n",
      "********************************************\n",
      "step: 390  loss: 8870.115234375  TRAIN_ACCURACY: 0.367\n",
      "********************************************\n",
      "step: 391  loss: 8868.239257812  TRAIN_ACCURACY: 0.367\n",
      "********************************************\n",
      "step: 392  loss: 8866.375976562  TRAIN_ACCURACY: 0.383\n",
      "********************************************\n",
      "step: 393  loss: 8864.525390625  TRAIN_ACCURACY: 0.417\n",
      "********************************************\n",
      "step: 394  loss: 8862.685546875  TRAIN_ACCURACY: 0.417\n",
      "********************************************\n",
      "step: 395  loss: 8860.859375000  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 396  loss: 8859.045898438  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 397  loss: 8857.244140625  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 398  loss: 8855.455078125  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 399  loss: 8853.676757812  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 400  loss: 8851.912109375  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 401  loss: 8850.159179688  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 402  loss: 8848.419921875  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 403  loss: 8846.690429688  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 404  loss: 8844.974609375  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 405  loss: 8843.269531250  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 406  loss: 8841.577148438  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 407  loss: 8839.896484375  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 408  loss: 8838.227539062  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 409  loss: 8836.569335938  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 410  loss: 8834.924804688  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 411  loss: 8833.291992188  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 412  loss: 8831.668945312  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 413  loss: 8830.058593750  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 414  loss: 8828.458984375  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 415  loss: 8826.872070312  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 416  loss: 8825.294921875  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 417  loss: 8823.731445312  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 418  loss: 8822.176757812  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 419  loss: 8820.634765625  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 420  loss: 8819.104492188  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 421  loss: 8817.583007812  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 422  loss: 8816.075195312  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 423  loss: 8814.578125000  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 424  loss: 8813.091796875  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 425  loss: 8811.616210938  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 426  loss: 8810.151367188  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 427  loss: 8808.697265625  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 428  loss: 8807.253906250  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 429  loss: 8805.822265625  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 430  loss: 8804.400390625  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 431  loss: 8802.989257812  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 432  loss: 8801.588867188  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 433  loss: 8800.199218750  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 434  loss: 8798.819335938  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 435  loss: 8797.452148438  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 436  loss: 8796.092773438  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 437  loss: 8794.744140625  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 438  loss: 8793.407226562  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 439  loss: 8792.079101562  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 440  loss: 8790.761718750  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 441  loss: 8789.454101562  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 442  loss: 8788.157226562  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 443  loss: 8786.871093750  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 444  loss: 8785.593750000  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 445  loss: 8784.325195312  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 446  loss: 8783.068359375  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 447  loss: 8781.821289062  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 448  loss: 8780.583007812  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 449  loss: 8779.354492188  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 450  loss: 8778.135742188  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 451  loss: 8776.925781250  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 452  loss: 8775.727539062  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 453  loss: 8774.537109375  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 454  loss: 8773.356445312  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 455  loss: 8772.185546875  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 456  loss: 8771.024414062  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 457  loss: 8769.870117188  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 458  loss: 8768.727539062  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 459  loss: 8767.591796875  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 460  loss: 8766.466796875  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 461  loss: 8765.349609375  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 462  loss: 8764.241210938  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 463  loss: 8763.143554688  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 464  loss: 8762.053710938  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 465  loss: 8760.971679688  TRAIN_ACCURACY: 0.400\n",
      "********************************************\n",
      "step: 466  loss: 8759.900390625  TRAIN_ACCURACY: 0.417\n",
      "********************************************\n",
      "step: 467  loss: 8758.836914062  TRAIN_ACCURACY: 0.417\n",
      "********************************************\n",
      "step: 468  loss: 8757.781250000  TRAIN_ACCURACY: 0.417\n",
      "********************************************\n",
      "step: 469  loss: 8756.735351562  TRAIN_ACCURACY: 0.417\n",
      "********************************************\n",
      "step: 470  loss: 8755.696289062  TRAIN_ACCURACY: 0.417\n",
      "********************************************\n",
      "step: 471  loss: 8754.666992188  TRAIN_ACCURACY: 0.417\n",
      "********************************************\n",
      "step: 472  loss: 8753.644531250  TRAIN_ACCURACY: 0.417\n",
      "********************************************\n",
      "step: 473  loss: 8752.630859375  TRAIN_ACCURACY: 0.417\n",
      "********************************************\n",
      "step: 474  loss: 8751.625976562  TRAIN_ACCURACY: 0.417\n",
      "********************************************\n",
      "step: 475  loss: 8750.628906250  TRAIN_ACCURACY: 0.417\n",
      "********************************************\n",
      "step: 476  loss: 8749.639648438  TRAIN_ACCURACY: 0.417\n",
      "********************************************\n",
      "step: 477  loss: 8748.659179688  TRAIN_ACCURACY: 0.417\n",
      "********************************************\n",
      "step: 478  loss: 8747.686523438  TRAIN_ACCURACY: 0.417\n",
      "********************************************\n",
      "step: 479  loss: 8746.720703125  TRAIN_ACCURACY: 0.417\n",
      "********************************************\n",
      "step: 480  loss: 8745.764648438  TRAIN_ACCURACY: 0.417\n",
      "********************************************\n",
      "step: 481  loss: 8744.814453125  TRAIN_ACCURACY: 0.417\n",
      "********************************************\n",
      "step: 482  loss: 8743.874023438  TRAIN_ACCURACY: 0.417\n",
      "********************************************\n",
      "step: 483  loss: 8742.939453125  TRAIN_ACCURACY: 0.417\n",
      "********************************************\n",
      "step: 484  loss: 8742.014648438  TRAIN_ACCURACY: 0.417\n",
      "********************************************\n",
      "step: 485  loss: 8741.094726562  TRAIN_ACCURACY: 0.417\n",
      "********************************************\n",
      "step: 486  loss: 8740.184570312  TRAIN_ACCURACY: 0.417\n",
      "********************************************\n",
      "step: 487  loss: 8739.280273438  TRAIN_ACCURACY: 0.417\n",
      "********************************************\n",
      "step: 488  loss: 8738.383789062  TRAIN_ACCURACY: 0.417\n",
      "********************************************\n",
      "step: 489  loss: 8737.494140625  TRAIN_ACCURACY: 0.417\n",
      "********************************************\n",
      "step: 490  loss: 8736.612304688  TRAIN_ACCURACY: 0.417\n",
      "********************************************\n",
      "step: 491  loss: 8735.737304688  TRAIN_ACCURACY: 0.417\n",
      "********************************************\n",
      "step: 492  loss: 8734.870117188  TRAIN_ACCURACY: 0.417\n",
      "********************************************\n",
      "step: 493  loss: 8734.009765625  TRAIN_ACCURACY: 0.417\n",
      "********************************************\n",
      "step: 494  loss: 8733.154296875  TRAIN_ACCURACY: 0.417\n",
      "********************************************\n",
      "step: 495  loss: 8732.308593750  TRAIN_ACCURACY: 0.417\n",
      "********************************************\n",
      "step: 496  loss: 8731.467773438  TRAIN_ACCURACY: 0.417\n",
      "********************************************\n",
      "step: 497  loss: 8730.634765625  TRAIN_ACCURACY: 0.417\n",
      "********************************************\n",
      "step: 498  loss: 8729.808593750  TRAIN_ACCURACY: 0.417\n",
      "********************************************\n",
      "step: 499  loss: 8728.988281250  TRAIN_ACCURACY: 0.417\n",
      "********************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "     \n",
    "    step = 1\n",
    "    while step * batch_size < training_epochs:\n",
    "        \n",
    "        sess.run(optimize, feed_dict = {x: x_train, y: y_train})\n",
    "        \n",
    "        if step % display_step == 0:\n",
    "            \n",
    "            los, acc = sess.run([loss, accuracy], feed_dict={x: x_train, y: y_train})\n",
    "            \n",
    "            train_loss[step * batch_size] = los\n",
    "            train_acc[step * batch_size] = acc\n",
    "            \n",
    "            print(\"step: %d  loss: %.9f  TRAIN_ACCURACY: %.3f\"  % (step, los, acc))\n",
    "            \n",
    "            print \"********************************************\"\n",
    "            \n",
    "        step = step + 1\n",
    "    \n",
    "    \n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEKCAYAAADaa8itAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHK/FNwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x122468310>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(211)\n",
    "plt.plot(train_loss, 'r')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"Training loss\")\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel('Training Accuracy')\n",
    "plt.ylim(0.0, 1)\n",
    "plt.plot(train_acc, 'r')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
